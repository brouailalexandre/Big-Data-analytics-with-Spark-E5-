{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)\n",
    "\n",
    "<p style=\"font-size:35px;font-weight: bold;color:blue\"> Lab 4 : Click-through rate machine learning pipeline</p>\n",
    "\n",
    "This lab covers the steps for creating a click-through rate (CTR) prediction pipeline.  You will work with the [Criteo Labs](http://labs.criteo.com/) dataset that was used for a recent [Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge). Note that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-size:21px;font-weight: bold;color:blue\">ESIEE, filière \"Data, réseaux et Internet des Objets\" </p>\n",
    "<p style=\"font-size:15px;font-weight: bold;color:blue\">DRIO-5101A (Automn 2017) -- Instructors: T. Vienne, J.-F. Bercher</p>\n",
    "\n",
    "**Sources**: These labs synthetize and *builds on* labs from several origins: \n",
    "- The series of moocs from Berkeley and Databricks,(Creative Commons licences), namely\n",
    "   - [Introduction to Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/info)\n",
    "   - [Big data Analysis with Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS110x+2T2016/info)\n",
    "   - [Distributed Machine Learning with Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS120x+2T2016/info)\n",
    "   - [Introduction to Big Data with Apache Spark](https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info)\n",
    "   - [Scalable Machine Learning](https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info)\n",
    "- [Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning](https://github.com/jadianes/spark-py-notebooks) (Apache License, Version 2.0)\n",
    "\n",
    "We have kept the labs text in english. This will enable us to reuse them in international sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20pt; font-weight: bold;color:blue\"> How to complete this lab :</p>\n",
    "\n",
    "This assignment is broken up into sections with bite-sized examples for demonstrating Spark functionalities. For each problem, you should start by thinking about the algorithm that you will use to *efficiently* process the log in a parallel, distributed manner. This means using the various [RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) operations along with [`lambda` functions](https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions) that are applied at each worker.\n",
    "\n",
    "+ *Part 1:* Featurize categorical data using one-hot-encoding.\n",
    "+ *Part 2:* Parse CTR data and generate OHE features with Spark ML.\n",
    "+ *Part 3:* First logistic regression model with Spark ML.\n",
    "+ *Part 4:* Reduce feature dimension via feature hashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prerequisites : Spark Context configuration \n",
    "\n",
    "<p>If you can't success to create SparContext and SQLContext objects such as explained on ESIEE icampus :<br/>\n",
    "Remove the following comments and modify the \"spark_path\" variable according to your spark location path.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# spark_path = \"/opt/spark-2.2.0-bin-hadoop2.7/\"\n",
    "# python_path = sys.executable\n",
    "# os.environ[\"SPARK_HOME\"] = spark_path\n",
    "# os.environ[\"HADOOP_HOME\"] = spark_path\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = python_path\n",
    "# sys.path.append(spark_path + \"python/lib/pyspark.zip\")\n",
    "# sys.path.append(spark_path + \"python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "# from pyspark import SparkConf, SparkContext, SQLContext\n",
    "# conf = SparkConf().set(\"master\", \"local[*]\").set(\"spark.app.name\", \"ESIEE\").set(\"spark.driver.host\", \"localhost\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PART 1 : Featurize categorical data using one-hot-encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 One-hot-encoding\n",
    "\n",
    "We would like to develop code to convert categorical features to numerical ones, and to build intuition, we will work with a sample unlabeled dataset with three data points, with each data point representing an animal. The first feature indicates the type of animal (bear, cat, mouse); the second feature describes the animal's color (black, tabby); and the third (optional) feature describes what the animal eats (mouse, salmon).\n",
    "\n",
    "In a one-hot-encoding (OHE) scheme, we want to represent each tuple of `(featureID, category)` via its own binary feature.  We can do this in Python by creating a dictionary that maps each tuple to a distinct integer, where the integer corresponds to a binary feature. To start, *manually enter the entries in the OHE dictionary associated with the sample dataset by mapping the tuples to consecutive integers* starting from zero,  ordering the tuples first by featureID and next by category.\n",
    "\n",
    "Later in this lab, we'll use OHE dictionaries to transform data points into compact lists of features that can be used in machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Data for manual OHE - Note: the first data point does not include any value for the optional third feature\n",
    "sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "sample_data_rdd = sc.parallelize([sample_one, sample_two, sample_three])\n",
    "\n",
    "# To fill : \n",
    "sample_ohe_dict_manual = {}\n",
    "sample_ohe_dict_manual[(0,'bear')] = 0\n",
    "sample_ohe_dict_manual[(0,'cat')] = 1\n",
    "sample_ohe_dict_manual[(0,'mouse')] = 2\n",
    "sample_ohe_dict_manual[(1,'black')] = 3\n",
    "sample_ohe_dict_manual[(1,'tabby')] = 4\n",
    "sample_ohe_dict_manual[(2,'mouse')] = 5\n",
    "sample_ohe_dict_manual[(2,'salmon')] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST One-hot-encoding (1.1)\n",
    "from databricks_test_helper import Test\n",
    "\n",
    "Test.assertEqualsHashed(sample_ohe_dict_manual[(0,'bear')],\n",
    "                        'b6589fc6ab0dc82cf12099d1c2d40ab994e8410c',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(0,'bear')]\")\n",
    "Test.assertEqualsHashed(sample_ohe_dict_manual[(0,'cat')],\n",
    "                        '356a192b7913b04c54574d18c28d46e6395428ab',\n",
    "                        u\"incorrect value for sample_ohe_dict_manual[(0,'cat')]\")\n",
    "Test.assertEqualsHashed(sample_ohe_dict_manual[(0,'mouse')],\n",
    "                        'da4b9237bacccdf19c0760cab7aec4a8359010b0',\n",
    "                        u\"incorrect value for sample_ohe_dict_manual[(0,'mouse')]\")\n",
    "Test.assertEqualsHashed(sample_ohe_dict_manual[(1,'black')],\n",
    "                        '77de68daecd823babbb58edb1c8e14d7106e83bb',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(1,'black')]\")\n",
    "Test.assertEqualsHashed(sample_ohe_dict_manual[(1,'tabby')],\n",
    "                        '1b6453892473a467d07372d45eb05abc2031647a',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(1,'tabby')]\")\n",
    "Test.assertEqualsHashed(sample_ohe_dict_manual[(2,'mouse')],\n",
    "                        'ac3478d69a3c81fa62e60f5c3696165a4e5e6ac4',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(2,'mouse')]\")\n",
    "Test.assertEqualsHashed(sample_ohe_dict_manual[(2,'salmon')],\n",
    "                        'c1dfd96eea8cc2b62785275bca38ac261256e278',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(2,'salmon')]\")\n",
    "Test.assertEquals(len(sample_ohe_dict_manual.keys()), 7,\n",
    "                  'incorrect number of keys in sample_ohe_dict_manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Sparse vectors \n",
    "\n",
    "Data points can typically be represented with a small number of non-zero OHE features relative to the total number of features that occur in the dataset.  By leveraging this sparsity and using sparse vector representations of OHE data, we can reduce storage and computational burdens.  Below are a few sample vectors represented as dense numpy arrays.  Use [SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector) to represent them in a sparse fashion, and verify that both the sparse and dense representations yield the same results when computing [dot products](http://en.wikipedia.org/wiki/Dot_product) (we will later use MLlib to train classifiers via gradient descent, and MLlib will need to compute dot products between SparseVectors and dense parameter vectors).\n",
    "\n",
    "Use `SparseVector(size, *args)` to create a new sparse vector where size is the length of the vector and args is either a dictionary, a list of (index, value) pairs, or two separate arrays of indices and values (sorted by index).  You'll need to create a sparse vector representation of each dense vector `aDense` and `bDense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3\n",
      "7.3\n",
      "-0.5\n",
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "a_dense = np.array([0., 3., 0., 4.])\n",
    "a_sparse = SparseVector(4, [1, 3], [3.0, 4.0])\n",
    "\n",
    "b_dense = np.array([0., 0., 0., 1.])\n",
    "b_sparse = SparseVector(4, [3], [1.0])\n",
    "\n",
    "w = np.array([0.4, 3.1, -1.4, -.5])\n",
    "print(a_dense.dot(w))\n",
    "print(a_sparse.dot(w))\n",
    "print(b_dense.dot(w))\n",
    "print(b_sparse.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Sparse Vectors (1.2)\n",
    "Test.assertTrue(isinstance(a_sparse, SparseVector), 'aSparse needs to be an instance of SparseVector')\n",
    "Test.assertTrue(isinstance(b_sparse, SparseVector), 'aSparse needs to be an instance of SparseVector')\n",
    "Test.assertTrue(a_dense.dot(w) == a_sparse.dot(w),\n",
    "                'dot product of aDense and w should equal dot product of aSparse and w')\n",
    "Test.assertTrue(b_dense.dot(w) == b_sparse.dot(w),\n",
    "                'dot product of bDense and w should equal dot product of bSparse and w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 OHE features as sparse vectors \n",
    "\n",
    "Now let's see how we can represent the OHE features for points in our sample dataset. Manually define OHE features for the three sample data points using `SparseVector` format.  Any feature that occurs in a point should have the value 1.0.  For example, the `DenseVector` for a point with features 2 and 4 would be `[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Reminder of the sample features\n",
    "# sampleOne = [(0, 'mouse'), (1, 'black')]\n",
    "# sampleTwo = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sampleThree =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "\n",
    "# To fill :\n",
    "sample_one_ohe_feat_manual = SparseVector(7,[2, 3], [1.0, 1.0])\n",
    "sample_two_ohe_feat_manual = SparseVector(7,[1, 4, 5], [1.0, 1.0, 1.0])\n",
    "sample_three_ohe_feat_manual = SparseVector(7,[0, 3, 6], [1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST OHE Features as sparse vectors (1.3)\n",
    "Test.assertTrue(isinstance(sample_one_ohe_feat_manual, SparseVector),\n",
    "                'sampleOneOHEFeatManual needs to be a SparseVector')\n",
    "Test.assertTrue(isinstance(sample_two_ohe_feat_manual, SparseVector),\n",
    "                'sampleTwoOHEFeatManual needs to be a SparseVector')\n",
    "Test.assertTrue(isinstance(sample_three_ohe_feat_manual, SparseVector),\n",
    "                'sampleThreeOHEFeatManual needs to be a SparseVector')\n",
    "Test.assertEqualsHashed(sample_one_ohe_feat_manual,\n",
    "                        'ecc00223d141b7bd0913d52377cee2cf5783abd6',\n",
    "                        'incorrect value for sampleOneOHEFeatManual')\n",
    "Test.assertEqualsHashed(sample_two_ohe_feat_manual,\n",
    "                        '26b023f4109e3b8ab32241938e2e9b9e9d62720a',\n",
    "                        'incorrect value for sampleTwoOHEFeatManual')\n",
    "Test.assertEqualsHashed(sample_three_ohe_feat_manual,\n",
    "                        'c04134fd603ae115395b29dcabe9d0c66fbdc8a7',\n",
    "                        'incorrect value for sampleThreeOHEFeatManual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 : Parse CTR data and generate OHE features with Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Accept the procedure \n",
    "The data we use are obtained from Criteo. You should agree with   [Criteo agreement](http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/) and then download the data in the `data/lab4` directory. Ask the professor for the exact procedure. \n",
    "Note that the download could take a few minutes, depending upon your connection speed.\n",
    "The script below extracts the data to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is already available. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import tarfile\n",
    "\n",
    "baseDir = os.path.join('..')\n",
    "inputPath = os.path.join('data', 'lab4', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "inputDir = os.path.split(fileName)[0]\n",
    "\n",
    "def extractTar(check = False):\n",
    "    # Find the zipped archive and extract the dataset\n",
    "    tars = glob.glob(os.path.join('..','data', 'lab5', 'dac_sample*.tar.gz*'))\n",
    "    if check and len(tars) == 0:\n",
    "        return False\n",
    "\n",
    "    if len(tars) > 0:\n",
    "        try:\n",
    "            tarFile = tarfile.open(tars[0])\n",
    "        except tarfile.ReadError:\n",
    "            if not check:\n",
    "                print('Unable to open tar.gz file.  Check your if the file is present.')\n",
    "            return False\n",
    "\n",
    "        tarFile.extract('dac_sample.txt', path=inputDir)\n",
    "        print('Successfully extracted: dac_sample.txt')\n",
    "        return True\n",
    "    else:\n",
    "        print('Unknown error')\n",
    "        return False\n",
    "\n",
    "\n",
    "if os.path.isfile(fileName):\n",
    "    print('File is already available. Nothing to do.')\n",
    "elif extractTar(check = True):\n",
    "    print('tar.gz file was already available.')\n",
    "else:\n",
    "    # Download the file and store it in the same directory as this notebook\n",
    "    try:\n",
    "        #u = urllib.request.urlopen(url)\n",
    "        urllib.request.urlretrieve(url, os.path.basename(urllib.request.urlparse(url).path))\n",
    "    except IOError:\n",
    "        print('tar.gz not found. Please check or install again')\n",
    "\n",
    "    extractTar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading the data \n",
    "\n",
    "Once the agreement is accepted, you have access to the data. In this first task, we will load the data as a Spark SQL dataframe and casting the columns with the right type. \n",
    "\n",
    "Datasets are composed with following columns : <br/>\n",
    "- _Label_ - Target variable that indicates if an ad was clicked (1) or not (0).<br/>\n",
    "- _I1-I13_ - A total of 13 columns of integer features (mostly count features).<br/>\n",
    "- _C1-C26_ - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes. \n",
    "\n",
    "_Note_ : Because we are going to deal with a huge amount of features, we have to limit our dataframe to a total of about 10K rows. However, if we had a Hadoop cluster, it would be  possible to deal with a much larger amount of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|label|col_float_1|col_float_2|col_float_3|col_float_4|col_float_5|col_float_6|col_float_7|col_float_8|col_float_9|col_float_10|col_float_11|col_float_12|col_float_13|col_string_14|col_string_15|col_string_16|col_string_17|col_string_18|col_string_19|col_string_20|col_string_21|col_string_22|col_string_23|col_string_24|col_string_25|col_string_26|col_string_27|col_string_28|col_string_29|col_string_30|col_string_31|col_string_32|col_string_33|col_string_34|col_string_35|col_string_36|col_string_37|col_string_38|col_string_39|\n",
      "+-----+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  0.0|       null|       -2.0|        2.0|       null|    13662.0|      120.0|       78.0|        0.0|       15.0|        null|         3.0|        null|        null|     05db9164|     bce95927|     078b949c|     13508380|     4cf72387|     7e0ccccf|     50b436c9|     0b153874|     a73ee510|     b1ed2e73|     a0a5e9d7|     48b3ee16|     ee79db7b|     07d13a8f|     fec218c0|     5f0caf67|     e5ba7672|     04d863d5|     55dd3565|     b1252a9d|     0358ebda|         null|     423fab69|     45ab94c8|     e8b83407|     c84c4aec|\n",
      "|  0.0|       null|       -1.0|       null|       null|       null|       null|       null|        0.0|       null|        null|        null|        null|        null|     05db9164|     403ea497|     2cbec47f|     3e2bfbda|     4cf72387|         null|     84eefcc9|     64523cfa|     a73ee510|     3b08e48b|     742e9bd6|     21a23bfe|     9325eab4|     07d13a8f|     e3209fc2|     587267a3|     2005abd1|     a78bd508|     21ddcdc9|     5840adea|     c2a93b37|         null|     be7c41b4|     1793a828|     e8b83407|     2fede552|\n",
      "+-----+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- col_float_1: double (nullable = true)\n",
      " |-- col_float_2: double (nullable = true)\n",
      " |-- col_float_3: double (nullable = true)\n",
      " |-- col_float_4: double (nullable = true)\n",
      " |-- col_float_5: double (nullable = true)\n",
      " |-- col_float_6: double (nullable = true)\n",
      " |-- col_float_7: double (nullable = true)\n",
      " |-- col_float_8: double (nullable = true)\n",
      " |-- col_float_9: double (nullable = true)\n",
      " |-- col_float_10: double (nullable = true)\n",
      " |-- col_float_11: double (nullable = true)\n",
      " |-- col_float_12: double (nullable = true)\n",
      " |-- col_float_13: double (nullable = true)\n",
      " |-- col_string_14: string (nullable = true)\n",
      " |-- col_string_15: string (nullable = true)\n",
      " |-- col_string_16: string (nullable = true)\n",
      " |-- col_string_17: string (nullable = true)\n",
      " |-- col_string_18: string (nullable = true)\n",
      " |-- col_string_19: string (nullable = true)\n",
      " |-- col_string_20: string (nullable = true)\n",
      " |-- col_string_21: string (nullable = true)\n",
      " |-- col_string_22: string (nullable = true)\n",
      " |-- col_string_23: string (nullable = true)\n",
      " |-- col_string_24: string (nullable = true)\n",
      " |-- col_string_25: string (nullable = true)\n",
      " |-- col_string_26: string (nullable = true)\n",
      " |-- col_string_27: string (nullable = true)\n",
      " |-- col_string_28: string (nullable = true)\n",
      " |-- col_string_29: string (nullable = true)\n",
      " |-- col_string_30: string (nullable = true)\n",
      " |-- col_string_31: string (nullable = true)\n",
      " |-- col_string_32: string (nullable = true)\n",
      " |-- col_string_33: string (nullable = true)\n",
      " |-- col_string_34: string (nullable = true)\n",
      " |-- col_string_35: string (nullable = true)\n",
      " |-- col_string_36: string (nullable = true)\n",
      " |-- col_string_37: string (nullable = true)\n",
      " |-- col_string_38: string (nullable = true)\n",
      " |-- col_string_39: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data as dataframe : \n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "input_path= \"../data/lab4/dac_sample.txt\"\n",
    "col_0_14 = [\"label\"] + [\"col_float_%s\" % i for i in range(1, 14)]\n",
    "col_14_40 = [\"col_string_%s\" % i for i in range(14, 40)]\n",
    "schema = StructType([StructField(col, DoubleType(), nullable=True) for col in col_0_14] + [StructField(col, StringType(), nullable=True) for col in col_14_40])\n",
    "\n",
    "raw_df = sqlContext.read.csv(input_path, schema=schema, sep=\"\\t\").repartition(16).cache()\n",
    "raw_df, garbage_df = raw_df.randomSplit([0.1, 0.9], seed=37)\n",
    "raw_df.show(2)\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Deal with missing values\n",
    "\n",
    "There are many NaN values in this dataset. In order to success following features engineering, we need to deal with it during this task.\n",
    "\n",
    "- Using methods `.describe()` and `.show()`, get an overview of the number of NaN values for columns `col_float_3`, `col_float_12` and `col_string_29`. Manually assign the number of missing values to below variables.\n",
    "- For each column in the dataset, fill in the missing values with the column mean if the column is double type or with value \"null\" if the column is string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+------------------+-------------+\n",
      "|summary|              label|      col_float_3|      col_float_12|col_string_29|\n",
      "+-------+-------------------+-----------------+------------------+-------------+\n",
      "|  count|              10001|             8103|              2242|         9600|\n",
      "|   mean|0.22107789221077892|47.57768727631741|0.9598572702943801|     Infinity|\n",
      "| stddev|0.41499358794106295|558.5352359792487| 5.498707290146208|          NaN|\n",
      "|    min|                0.0|              0.0|               0.0|     0012a4bb|\n",
      "|    max|                1.0|          21486.0|             159.0|     ffefde52|\n",
      "+-------+-------------------+-----------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# To fill manually : \n",
    "raw_df.select(\"label\", \"col_float_3\", \"col_float_12\", \"col_string_29\").describe().show()\n",
    "null_col_float_3 = 1910\n",
    "null_col_float_12 = 7740\n",
    "null_col_string_29 = 377\n",
    "\n",
    "# Fill the null values : \n",
    "from pyspark.sql.functions import mean\n",
    "for col in raw_df.columns:\n",
    "    if \"float\" in col: # If type double\n",
    "        raw_df = raw_df.na.fill({col: raw_df.select(mean(col)).collect()[0][0]})\n",
    "    elif \"string\" in col: \n",
    "        raw_df = raw_df.na.fill({col: \"null\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.select('col_float_3').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Dealing with missing values (2.2)\n",
    "describe_df = raw_df.describe()\n",
    "summary_count = describe_df.filter(describe_df.summary==\"count\").collect()[0]\n",
    "Test.assertEquals(null_col_float_3, 1910, \"This is not the right number of null values for col_float_3\")\n",
    "Test.assertEquals(null_col_float_12, 7740, \"This is not the right number of null values for col_float_12\") \n",
    "Test.assertEquals(null_col_string_29, 377, \"This is not the right number of null values for col_string_29\") \n",
    "Test.assertEquals(list(summary_count), [\"count\"] + 40 * [\"9999\"], \"There is still missing values\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Splitting datasets\n",
    "\n",
    "We are now ready to start working with the actual CTR data, and our first task involves splitting it into training, validation, and test datasets.  \n",
    "\n",
    "- First, make the splitting, use the specified weights and seed to create datasets.\n",
    "- Then _cache_ and _repartition_ on 16 threads each of these dataframes, as we will be accessing them multiple times in the remainder of this lab.\n",
    "- Finally, compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8026 1034 941 10001\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Train-Val-Test split \n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "df_train, df_val, df_test = raw_df.randomSplit(weights,seed)\n",
    "\n",
    "# Cache the data\n",
    "df_train = df_train.repartition(16).cache()\n",
    "df_val = df_val.repartition(16).cache()\n",
    "df_test = df_test.repartition(16).cache()\n",
    "\n",
    "n_train = df_train.count()\n",
    "n_val = df_val.count()\n",
    "n_test = df_test.count()\n",
    "print(n_train, n_val, n_test, n_train + n_val + n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test failed. incorrect value for n_train\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Splitting datasets (2.4)\n",
    "Test.assertTrue(all([df_train.is_cached, df_val.is_cached, df_test.is_cached]),\n",
    "                'you must cache the split data')\n",
    "Test.assertEquals(n_train, 8024, 'incorrect value for n_train')\n",
    "Test.assertEquals(n_val, 1034, 'incorrect value for n_val')\n",
    "Test.assertEquals(n_test, 941, 'incorrect value for n_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 One Hot Encoding in Spark ML\n",
    "\n",
    "In Spark ML, it already exists estimators and transformers to one hot encode dataframes columns. We will practice on this below example dataframe containing different colors.\n",
    "\n",
    "- First, we have to index this categorical variable. Using a [StringIndexer](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer), create a new column named `color_indexed`.\n",
    "- Then, one hot encode the indexed variable using a [OneHotEncoder](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder) transformer. Add a new column named `color_encoded`.\n",
    "- Finally, display the first 5 rows of your new encoded dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe : \n",
      "+------+\n",
      "| color|\n",
      "+------+\n",
      "|  blue|\n",
      "| black|\n",
      "| black|\n",
      "|  blue|\n",
      "|purple|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Creating random color data : \n",
    "color_data = 100 * [\"red\"] + 100 * [\"blue\"] + 100 * [\"green\"] + 100 * [\"purple\"] + 100 * [\"yellow\"] + 100 * [\"black\"]\n",
    "random.shuffle(color_data)\n",
    "random_data = [[el] for el in color_data]\n",
    "df_color = sqlContext.createDataFrame(data=random_data, schema=[\"color\"])\n",
    "print(\"Original dataframe : \")\n",
    "df_color.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after one hot encoding : \n",
      "+------+-------------+-------------+\n",
      "| color|color_indexed|color_encoded|\n",
      "+------+-------------+-------------+\n",
      "|  blue|          1.0|(5,[1],[1.0])|\n",
      "| black|          3.0|(5,[3],[1.0])|\n",
      "| black|          3.0|(5,[3],[1.0])|\n",
      "|  blue|          1.0|(5,[1],[1.0])|\n",
      "|purple|          0.0|(5,[0],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Indexing :\n",
    "color_indexer = StringIndexer(inputCol=\"color\", outputCol=\"color_indexed\")\n",
    "indexer_transformer = color_indexer.fit(df_color)\n",
    "df_color = indexer_transformer.transform(df_color)\n",
    "\n",
    "# One Hot Encoding : \n",
    "color_ohe = OneHotEncoder(inputCol=\"color_indexed\", outputCol=\"color_encoded\")\n",
    "df_color = color_ohe.transform(df_color)\n",
    "print(\"Dataframe after one hot encoding : \")\n",
    "df_color.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST One Hot Encoding in Spark ML (2.5)\n",
    "Test.assertTrue(\"color_indexed\" in df_color.columns, \"Missing column color_indexed.\")\n",
    "Test.assertTrue(\"color_encoded\" in df_color.columns, \"Missing column color_indexed.\")\n",
    "Test.assertEquals(len(df_color.select(\"color_encoded\").take(2)[0][0]), 5, \"Number of sparse vector elements has to be 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 One Hot Encoding all CTR columns\n",
    "\n",
    "We will now encode each of the categorical column in CTR train, validation and test datasets. for each categorical column, **X**, in training dataframe :\n",
    "\n",
    "- Create a `StringIndexer` instance with inputCol = `X` and outputCol = `X_indexed`. Make sure to specify parameter **handleInvalid=\"skip\"** to avoid unseen fetures issues in the following tasks.\n",
    "- Then, fit the `StringIndexer` on train dataframe and transform train, validation and test dataframes.\n",
    "- Then create a `OneHotEncoder` instance with inputCol = X_indexed and outputCol = X_encoded.\n",
    "- Using the one hot encoder, transform each of train, validation and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "for col in df_train.columns:\n",
    "    if \"string\" in col: # If categorical column :\n",
    "        # Indexing :\n",
    "        str_indexer = StringIndexer(inputCol=col, outputCol=\"%s_indexed\" % col, handleInvalid=\"skip\")\n",
    "        indexer_model = str_indexer.fit(df_train)\n",
    "        df_train = indexer_model.transform(df_train)\n",
    "        df_val = indexer_model.transform(df_val)\n",
    "        df_test = indexer_model.transform(df_test)\n",
    "        \n",
    "        # One Hot Encoding :\n",
    "        str_ohe = OneHotEncoder(inputCol=\"%s_indexed\" % col, outputCol=\"%s_encoded\" % col)\n",
    "        df_train = str_ohe.transform(df_train)\n",
    "        df_val = str_ohe.transform(df_val)\n",
    "        df_test = str_ohe.transform(df_test)\n",
    "    else: # If not categorical column :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST One Hot Encoding in Spark ML (2.6)\n",
    "Test.assertEquals(len(df_train.columns), 92, \"Too few or too much columns for train dataframe.\")\n",
    "Test.assertEquals(len(df_val.columns), 92, \"Too few or too much columns for train dataframe.\")\n",
    "Test.assertEquals(len(df_test.columns), 92, \"Too few or too much columns for train dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Assembling the features\n",
    "\n",
    "Before training a machine learning algorithm, we need to assemble features using a Spark ML [VectorAssembler](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n",
    "\n",
    "- Create a list object containing all columns names to assemble. Assign the result to variable `columns_to_assemble`.\n",
    "- Create a `VectorAssembler` instance and use it to add another column named `features` to train, validation and test dataframes.\n",
    "\n",
    "_Hint_ : You have to assemble all float columns and categorical encoded columns. Fortunately, these columns have some name patterns to easily select it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns to assemble :\n",
      "['col_float_1', 'col_float_2', 'col_float_3', 'col_float_4', 'col_float_5', 'col_float_6', 'col_float_7', 'col_float_8', 'col_float_9', 'col_float_10', 'col_float_11', 'col_float_12', 'col_float_13', 'col_string_14_encoded', 'col_string_15_encoded', 'col_string_16_encoded', 'col_string_17_encoded', 'col_string_18_encoded', 'col_string_19_encoded', 'col_string_20_encoded', 'col_string_21_encoded', 'col_string_22_encoded', 'col_string_23_encoded', 'col_string_24_encoded', 'col_string_25_encoded', 'col_string_26_encoded', 'col_string_27_encoded', 'col_string_28_encoded', 'col_string_29_encoded', 'col_string_30_encoded', 'col_string_31_encoded', 'col_string_32_encoded', 'col_string_33_encoded', 'col_string_34_encoded', 'col_string_35_encoded', 'col_string_36_encoded', 'col_string_37_encoded', 'col_string_38_encoded', 'col_string_39_encoded']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Select columns to Assemble :\n",
    "columns_to_assemble = [col for col in df_train.columns if \"float\" in col or \"encoded\" in col]\n",
    "print(\"Selected columns to assemble :\")\n",
    "print(columns_to_assemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'features' in df_train.columns: df_train = df_train.drop('features')\n",
    "if 'features' in df_val.columns: df_val = df_val.drop('features')\n",
    "if 'features' in df_test.columns: df_test = df_test.drop('features')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the columns\n",
    "vect_assembler = VectorAssembler(inputCols=columns_to_assemble, outputCol=\"features\")\n",
    "df_train = vect_assembler.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = vect_assembler.transform(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = vect_assembler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train dataframe :\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(39101,[2,3,4,5,6...|\n",
      "|  0.0|(39101,[2,3,4,5,6...|\n",
      "|  0.0|(39101,[1,2,3,4,5...|\n",
      "|  0.0|(39101,[1,2,3,4,5...|\n",
      "|  0.0|(39101,[1,2,3,4,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final train dataframe :\")\n",
    "df_train.select(\"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST One Hot Encoding in Spark ML (2.7)\n",
    "Test.assertEquals(columns_to_assemble, ['col_float_1', 'col_float_2', 'col_float_3', 'col_float_4', 'col_float_5', 'col_float_6', 'col_float_7', 'col_float_8', 'col_float_9', 'col_float_10', 'col_float_11', 'col_float_12', 'col_float_13', 'col_string_14_encoded', 'col_string_15_encoded', 'col_string_16_encoded', 'col_string_17_encoded', 'col_string_18_encoded', 'col_string_19_encoded', 'col_string_20_encoded', 'col_string_21_encoded', 'col_string_22_encoded', 'col_string_23_encoded', 'col_string_24_encoded', 'col_string_25_encoded', 'col_string_26_encoded', 'col_string_27_encoded', 'col_string_28_encoded', 'col_string_29_encoded', 'col_string_30_encoded', 'col_string_31_encoded', 'col_string_32_encoded', 'col_string_33_encoded', 'col_string_34_encoded', 'col_string_35_encoded', 'col_string_36_encoded', 'col_string_37_encoded', 'col_string_38_encoded', 'col_string_39_encoded'], \"Wrong selected columns\")\n",
    "Test.assertTrue(\"features\" in df_train.columns, \"missing features column in train dataframe.\")\n",
    "Test.assertTrue(\"features\" in df_val.columns, \"missing features column in val dataframe.\")\n",
    "Test.assertTrue(\"features\" in df_test.columns, \"missing features column in test dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 : First logistic regression with Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Logistic regression\n",
    "\n",
    "We are now ready to train our first CTR classifier. A natural classifier to use in this setting is logistic regression, since it models the probability of a click-through event rather than returning a binary response, and when working with rare events, probabilistic predictions are useful. \n",
    "\n",
    "- First, create a [LogisticRegression](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression) instance. Set the hyperameters such as defined in below note.\n",
    "- Then, train the machine learning algorithm and assign the resulting model to variable _model_.\n",
    "- Finally, make predictions on both train and validation set. Assign the resulting dataframe to variables **df_train_pred** and **df_val_pred**. Print the validation dataframe on the standard output using method _.show()_.\n",
    "\n",
    "_Note_ : Because we deal with a huge amount of different features, it is highly recommended to use regularization to avoid overfitting. \n",
    "\n",
    "_Note_ : Concerning logistic regression hyperparameters, use a L2 regularization with strength 10 and 2 maximum iterations and set _fitIntercept_ to True. Other hyperparameters have to be set default.\n",
    "\n",
    "_Note_ : Remember that after each action, Spark RDD and dataframes are automatically uncached. Because a machine learning algorithm is a heavy process, don't forget to `cache` again your train dataframe before training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Train a logistic regression on train dataframe : \n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", fitIntercept=True, maxIter=2,\n",
    "                        elasticNetParam=1.0, regParam=10)\n",
    "model = lr.fit(df_train.cache())\n",
    "\n",
    "# Make prediction on validation dataframe and show it :\n",
    "df_train_pred = model.transform(df_train.cache())\n",
    "df_val_pred = model.transform(df_val.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Logistic Regression (3.1)\n",
    "Test.assertEquals(df_train.is_cached, True, \"do not forget to cache train dataframe.\")\n",
    "Test.assertEquals(lr.getOrDefault(\"labelCol\"), \"label\", \"incorect labelCol parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"featuresCol\"), \"features\", \"incorect featuresCol parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"fitIntercept\"), True, \"incorect fitIntercept parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"maxIter\"), 2, \"incorect maxIter parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"elasticNetParam\"), 1.0, \"incorect elasticNetParam parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"regParam\"), 10, \"incorect regParam parameter\")\n",
    "Test.assertTrue(\"probability\" in df_train_pred.columns, \"missing transformation for train dataframe.\")\n",
    "Test.assertTrue(\"probability\" in df_val_pred.columns, \"missing transformation for validation dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define the Log Loss metrics\n",
    "\n",
    "Throughout this lab, we will use log loss to evaluate the quality of models.  Log loss is defined as:  \\begin{align} \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\end{align}  where $ \\scriptsize p$ is a probability between 0 and 1 and $ \\scriptsize y$ is a label of either 0 or 1. Log loss is a standard evaluation criterion when predicting rare-events such as click-through rate prediction (it is also the criterion used in the [Criteo Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge)).  Write an UDF function to compute log loss between a probability and a label, and evaluate it on some sample inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.01005033585350145\n",
      "4.605170185988091\n",
      "4.605170185988091\n",
      "0.01005033585350145\n",
      "25.328436022934504\n",
      "1.000000082745371e-11\n",
      "25.328435940194137\n",
      "+------------+-----+--------------------+\n",
      "| probability|label|            log_loss|\n",
      "+------------+-----+--------------------+\n",
      "|[0.02, 0.98]|  1.0|0.020202707317519466|\n",
      "|[0.87, 0.13]|  0.0| 0.13926206733350766|\n",
      "+------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from math import log\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def compute_log_loss(p_tuple, y):\n",
    "    \"\"\"\n",
    "    Computes the value of log loss for a given observation (probability + label).\n",
    "    Args:\n",
    "    p_tuple (2 element tuple): p_tuple = [1-p, p] where p the is probability that obervation is class 1.\n",
    "    y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    p = p_tuple[1]\n",
    "    epsilon = 10e-12\n",
    "    if p==0: p=epsilon\n",
    "    if p==1: p=1-epsilon    \n",
    "    return -log(p) if y==1 else -log(1-p) \n",
    "\n",
    "# Test the function : \n",
    "print(compute_log_loss([.5, .5], 1))\n",
    "print(compute_log_loss([.5, .5], 0))\n",
    "print(compute_log_loss([0.01, .99], 1))\n",
    "print(compute_log_loss([0.01, .99], 0))\n",
    "print(compute_log_loss([.99, 0.01], 1))\n",
    "print(compute_log_loss([.99, 0.01], 0))\n",
    "print(compute_log_loss([1.0, 0.0], 1))\n",
    "print(compute_log_loss([0.0, 1.0], 1))\n",
    "print(compute_log_loss([0.0, 1.0], 0))\n",
    "\n",
    "# Wrap the function into an UDF : \n",
    "my_udf = udf(lambda proba, label : compute_log_loss(proba, label), DoubleType())\n",
    "\n",
    "# Test on really small dataset : \n",
    "my_df_test = sqlContext.createDataFrame(data=[[[0.02, 0.98], 1.0], [[0.87, 0.13], 0.0]],\n",
    "                                        schema=[\"probability\", \"label\"])\n",
    "my_df_test = my_df_test.withColumn(\"log_loss\", my_udf(\"probability\", \"label\"))\n",
    "my_df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Log loss (3.2)\n",
    "Test.assertTrue(np.allclose([compute_log_loss([.5, .5], 1), compute_log_loss([0.99, 0.01], 0), compute_log_loss([0.99, 0.01], 1)],\n",
    "                            [0.69314718056, 0.0100503358535, 4.60517018599]),\n",
    "                'computeLogLoss is not correct')\n",
    "Test.assertTrue(np.allclose([compute_log_loss([1.0, 0.0], 1), compute_log_loss([0.0, 1.0], 1), compute_log_loss([0.0, 1.0], 0)],\n",
    "                            [25.3284360229, 1.00000008275e-11, 25.3284360229]),\n",
    "                'computeLogLoss needs to bound p away from 0 and 1 by epsilon')\n",
    "\n",
    "Test.assertEquals([round(el[0], 2) for el in my_df_test.select(\"log_loss\").collect()], [0.020, 0.14], \"Incorrect values for dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate the model \n",
    "\n",
    "Now that we have trained a logistic regression algorithm and made predictions on both train and validation datasets, we can now evaluate the algorithm performs. Compute the mean log loss for both dataframes and assign results to variables `mean_log_loss_train` and `mean_log_loss_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log loss on train dataset : 0.5277393350582085\n",
      "Mean log loss on validation dataset : 0.5169674762842519\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df_train_pred_logloss = df_train_pred.withColumn(\"logloss\", my_udf(\"probability\", \"label\"))\n",
    "mean_log_loss_train = df_train_pred_logloss.select(mean(df_train_pred_logloss.logloss)).collect()[0][0]\n",
    "\n",
    "df_val_pred_logloss = df_val_pred.withColumn(\"logloss\", my_udf(\"probability\", \"label\"))\n",
    "mean_log_loss_val = df_val_pred_logloss.select(mean(df_val_pred_logloss.logloss)).collect()[0][0]\n",
    "\n",
    "print(\"Mean log loss on train dataset : %s\" % mean_log_loss_train)\n",
    "print(\"Mean log loss on validation dataset : %s\" % mean_log_loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Log loss (3.3)\n",
    "Test.assertEquals(round(mean_log_loss_train, 2), 0.53, \"Incorrect mean log loss for train dataset\")\n",
    "Test.assertEquals(round(mean_log_loss_val, 2), 0.53, \"Incorrect mean log loss for validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Visualization 2: ROC curve\n",
    "\n",
    "We will now visualize how well the model predicts our target.  To do this we generate a plot of the ROC curve.  The ROC curve shows us the trade-off between the false positive rate and true positive rate, as we liberalize the threshold required to predict a positive outcome.  A random model is represented by the dashed line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAF6CAYAAAAnNj0FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2ojBFxEREVEb/QWY0RVX0awArwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73a582d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_plot(xticks, yticks, figsize=(10, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "labelsAndWeights = [[el[0], el[1][1]] for el in df_val_pred_logloss.select(\"label\", \"probability\").collect()]\n",
    "labelsAndWeights.sort(key=lambda kv: kv[1], reverse=True)\n",
    "labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n",
    "\n",
    "length = labelsByWeight.size\n",
    "truePositives = labelsByWeight.cumsum()\n",
    "numPositive = truePositives[-1]\n",
    "falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n",
    "\n",
    "truePositiveRate = truePositives / numPositive\n",
    "falsePositiveRate = falsePositives / (length - numPositive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4 : Reduce feature dimension via feature hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Hash function \n",
    "\n",
    "As we just saw, using a one-hot-encoding featurization can yield a model with good statistical accuracy.  However, the number of distinct categories across all features is quite large -- recall that we observed  categories in the training data in Part 3.  Moreover, the full Kaggle training dataset includes more than distinct categories, and the 38K Kaggle dataset itself is just a small subset of Criteo's labeled data.  Hence, featurizing via a one-hot-encoding representation would lead to a very large feature vector. To reduce the dimensionality of the feature space, we will use `feature hashing`. See [an explanation here](https://www.quora.com/Can-you-explain-feature-hashing-in-an-easily-understandable-way)\n",
    "\n",
    "Below is the hash function that we will use to discover features hashing. We will first use this hash function with the three sample data points from Part 1 to gain some intuition.  Specifically :\n",
    "- Run code to hash the three sample points with `numBuckets` set to `4`. \n",
    "- Run code to hash the three sample points with `numBuckets` set to `100`. \n",
    "- Observe the resulting hashed feature dictionaries. What do you think of it ?\n",
    "\n",
    "_Note_ : Don't forget to set `printMapping` on `True` to understand logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def hash_function(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString.encode('utf-8')).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print(mapping)\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "# Reminder of the sample values:\n",
    "# sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features hashing with two features :\n",
      "{'mouse0': 3, 'black1': 2}\n",
      "{'tabby1': 0, 'mouse2': 2, 'cat0': 0}\n",
      "{'bear0': 0, 'black1': 2, 'salmon2': 1}\n",
      "\n",
      " Features hashing with one hundred features :\n",
      "{'mouse0': 31, 'black1': 14}\n",
      "{'tabby1': 16, 'mouse2': 62, 'cat0': 40}\n",
      "{'bear0': 72, 'black1': 14, 'salmon2': 5}\n",
      "\n",
      "\t\t 4 Buckets \t\t\t 100 Buckets\n",
      "SampleOne:\t {2: 1.0, 3: 1.0}\t\t {14: 1.0, 31: 1.0}\n",
      "SampleTwo:\t {0: 2.0, 2: 1.0}\t\t {16: 1.0, 40: 1.0, 62: 1.0}\n",
      "SampleThree:\t {0: 1.0, 1: 1.0, 2: 1.0}\t {72: 1.0, 5: 1.0, 14: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Use four buckets\n",
    "print(\"Features hashing with two features :\")\n",
    "samp_one_four_buckets = hash_function(4, sample_one, True)\n",
    "samp_two_four_buckets = hash_function(4, sample_two, True)\n",
    "samp_three_four_buckets = hash_function(4, sample_three, True)\n",
    "\n",
    "# Use one hundred buckets\n",
    "print(\"\\n Features hashing with one hundred features :\")\n",
    "samp_one_hundred_buckets = hash_function(100, sample_one, True)\n",
    "samp_two_hundred_buckets = hash_function(100, sample_two, True)\n",
    "samp_three_hundred_buckets = hash_function(100, sample_three, True)\n",
    "\n",
    "print('\\n\\t\\t 4 Buckets \\t\\t\\t 100 Buckets')\n",
    "print('SampleOne:\\t {0}\\t\\t {1}'.format(samp_one_four_buckets, samp_one_hundred_buckets))\n",
    "print('SampleTwo:\\t {0}\\t\\t {1}'.format(samp_two_four_buckets, samp_two_hundred_buckets))\n",
    "print('SampleThree:\\t {0}\\t {1}'.format(samp_three_four_buckets, samp_three_hundred_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Hash function (4.1)\n",
    "Test.assertEquals(samp_one_four_buckets, {2: 1.0, 3: 1.0}, 'incorrect value for samp_one_four_buckets')\n",
    "Test.assertEquals(samp_three_hundred_buckets, {72: 1.0, 5: 1.0, 14: 1.0},\n",
    "                  'incorrect value for samp_three_hundred_buckets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Features hashing in Spark ML\n",
    "\n",
    "In Spark ML, it already exists a features hasher based on Austin Appleby’s MurmurHash 3 algorithm [HashingTF](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF). Note that its using is not really convenient and requires to wrap all string features into an array. The following code applies the Hashing TF on the previous color dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe : \n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|color_1|color_2|color_3|color_4|color_5|color_6|color_7|color_8|color_9|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|  brown|   pink|  black|  white|  brown|  green|   pink|  black|   blue|\n",
      "|   grey|   pink| orange|   grey|  brown| purple| purple|  green| purple|\n",
      "|   pink| orange| purple|  green|  white|   blue|  green|    red|  green|\n",
      "| purple|  brown|  white|  black|  white|  black|  brown|   grey| purple|\n",
      "|  green| orange|  green|   grey|  black|  green|  white|   pink| yellow|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After Hashing TF : \n",
      "+--------------------+--------------------+\n",
      "|         color_array|        color_hashed|\n",
      "+--------------------+--------------------+\n",
      "|[brown, pink, bla...|(8,[0,1,5],[6.0,6...|\n",
      "|[grey, pink, oran...|(8,[0,1,3,5,7],[2...|\n",
      "|[pink, orange, pu...|(8,[0,1,3,5],[2.0...|\n",
      "|[purple, brown, w...|(8,[0,3,5,7],[4.0...|\n",
      "|[green, orange, g...|(8,[1,4,5,7],[8.0...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, split\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "### Creating random color data - 11 different colors - 9 columns - 100 rows : \n",
    "all_colors = [\"red\", \"blue\", \"green\", \"purple\", \"yellow\", \"black\", \"brown\", \"grey\", \"pink\", \"orange\", \"white\"]\n",
    "random_colors_data = [[random.choice(all_colors) for i in range(1,10)] for j in range(0,100)]\n",
    "random_colors_header = [\"color_%s\" % i for i in range(1,10)]\n",
    "df_color = sqlContext.createDataFrame(data=random_colors_data, schema=random_colors_header)\n",
    "print(\"Original dataframe : \")\n",
    "df_color.show(5)\n",
    "\n",
    "### Wrap all string color into an Array named \"all_colors\" :\n",
    "df_color = df_color.withColumn(\"color_array\", df_color.color_1) # First step : taking first column\n",
    "all_cols_except_0 = [color for color in df_color.columns if color != \"color_1\"]\n",
    "for col in all_cols_except_0: # Concat all strings\n",
    "    df_color = df_color.withColumn(\"color_array\", concat_ws(',', df_color.color_array, df_color[\"%s\" % col]))\n",
    "df_color = df_color.withColumn(\"color_array\", split(df_color.color_array, \",\").cast(ArrayType(StringType())))\n",
    "\n",
    "# Spark ML Hashing TF - 4 features:\n",
    "hasher = HashingTF(inputCol=\"color_array\", numFeatures=8, outputCol=\"color_hashed\")\n",
    "df_color = hasher.transform(df_color)\n",
    "print(\"After Hashing TF : \")\n",
    "df_color.select(\"color_array\", \"color_hashed\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Features Hashing on CTR dataframes\n",
    "\n",
    "It is now your turn to apply features hashing on aall CTR categorical columns. To achieve this task, feel free to refer to previous question in order to correctly use the Spark ML `HashingTF`.\n",
    "\n",
    "- First, select all the **string** columns to concat except column `col_string_14`. Assign the resulting to variable `all_cols_except_14`.\n",
    "- Then, for each of train, validation and test dataframes, create a new column named `string_array` which contains all concated categorical columns. See previous question to achieve this task.\n",
    "- Finally, create a HashingTF instance with `numFeatures=32768`. Than apply this features hasher on train, validation and test dataframes to create new column named `string_array_hashed`.\n",
    "\n",
    "_Note_ : In case Spark displays log \"column already exists...\", you can run the following cell to drop the already created columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case of issue \"column already exists...\"\n",
    "df_train = df_train.drop(\"string_array_hashed\")\n",
    "df_val = df_val.drop(\"string_array_hashed\")\n",
    "df_test = df_test.drop(\"string_array_hashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Hashing TF : \n",
      "+-----+--------------------+--------------------+\n",
      "|label|        string_array| string_array_hashed|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|[05db9164, 1cfdf7...|(32768,[4583,5490...|\n",
      "|  0.0|[05db9164, 3f0d3f...|(32768,[857,2902,...|\n",
      "|  0.0|[05db9164, 89ddfe...|(32768,[414,1936,...|\n",
      "|  0.0|[05db9164, 58e67a...|(32768,[316,685,7...|\n",
      "|  0.0|[5a9ed9b0, 4c2bc5...|(32768,[857,2657,...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, split\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# Select all columns to concat except `col_string_14` :\n",
    "all_cols_except_14 = [\"col_string_%s\" % i for i in range(15, 40)]\n",
    "\n",
    "# Process Train Dataframe\n",
    "df_train = df_train.withColumn(\"string_array\", df_train.col_string_14) # Create empty col\n",
    "for col in all_cols_except_14: # Concat all strings\n",
    "    df_train = df_train.withColumn(\"string_array\", concat_ws(',', df_train.string_array, df_train[\"%s\" % col]))\n",
    "df_train = df_train.withColumn(\"string_array\", split(df_train.string_array, \",\").cast(ArrayType(StringType())))\n",
    "\n",
    "# Process Validation Dataframe\n",
    "df_val = df_val.withColumn(\"string_array\", df_val.col_string_14) # Create empty col\n",
    "for col in all_cols_except_14: # Concat all strings\n",
    "    df_val = df_val.withColumn(\"string_array\", concat_ws(',', df_val.string_array, df_val[\"%s\" % col]))\n",
    "df_val = df_val.withColumn(\"string_array\", split(df_val.string_array, \",\").cast(ArrayType(StringType())))\n",
    "\n",
    "# Process Train Dataframe\n",
    "df_test = df_test.withColumn(\"string_array\", df_test.col_string_14) # Create empty col\n",
    "for col in all_cols_except_14: # Concat all strings\n",
    "    df_test = df_test.withColumn(\"string_array\", concat_ws(',', df_test.string_array, df_test[\"%s\" % col]))\n",
    "df_test = df_test.withColumn(\"string_array\", split(df_test.string_array, \",\").cast(ArrayType(StringType())))\n",
    "\n",
    "# Spark ML Hashing TF - 32768 features:\n",
    "hasher = HashingTF(inputCol=\"string_array\", numFeatures=32768, outputCol=\"string_array_hashed\")\n",
    "df_train = hasher.transform(df_train.cache())\n",
    "df_val = hasher.transform(df_val.cache())\n",
    "df_test = hasher.transform(df_test.cache())\n",
    "print(\"After Hashing TF : \")\n",
    "df_train.select(\"label\", \"string_array\", \"string_array_hashed\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Features hashing on CTR dataframes (4.3)\n",
    "Test.assertTrue(\"string_array\" in df_train.columns, \"missing string_array col in train dataframe\")\n",
    "Test.assertTrue(\"string_array\" in df_val.columns, \"missing string_array col in validation dataframe\")\n",
    "Test.assertTrue(\"string_array\" in df_test.columns, \"missing string_array col in test dataframe\")\n",
    "Test.assertTrue(\"string_array_hashed\" in df_train.columns, \"missing string_array_hashed col in train dataframe\")\n",
    "Test.assertTrue(\"string_array_hashed\" in df_val.columns, \"missing string_array_hashed col in validation dataframe\")\n",
    "Test.assertTrue(\"string_array_hashed\" in df_test.columns, \"missing string_array_hashed col in test dataframe\")\n",
    "Test.assertEquals(len(df_train.select(\"string_array\").take(2)[0][0]), 26, \"train df string_array expects 26 elements\")\n",
    "Test.assertEquals(len(df_val.select(\"string_array\").take(2)[0][0]), 26, \"val df string_array expects 26 elements\")\n",
    "Test.assertEquals(len(df_test.select(\"string_array\").take(2)[0][0]), 26, \"test df string_array expects 26 elements\")\n",
    "Test.assertEquals(len(df_train.select(\"string_array_hashed\").take(2)[0][0]), 32768, \"train df string_array_hashed expects 32768 elements\")\n",
    "Test.assertEquals(len(df_val.select(\"string_array_hashed\").take(2)[0][0]), 32768, \"val df string_array_hashed expects 32768 elements\")\n",
    "Test.assertEquals(len(df_test.select(\"string_array_hashed\").take(2)[0][0]), 32768, \"test df string_array_hashed expects 32768 elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Vector Assembler\n",
    "\n",
    "Let us now assemble our new features before training machine learning algorithm. Using a `VectorAssembler`, create a new column named `features_2` for each of train, validation and test dataframes. Column `features_2` has to contain column `string_array_hashed` and all the continuous (float) columns. \n",
    "\n",
    "- Select all the columns names to assemble and assign it to variable `columns_to_assemble`.\n",
    "- Define a VectorAssembler instance and use it on each of train, validation and test dataframes.\n",
    "\n",
    "_Note_ : In case Spark displays log \"column already exists...\", you can run the following cell to drop the already created columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case of issue \"column already exists...\"\n",
    "df_train = df_train.drop(\"features_2\")\n",
    "df_val = df_val.drop(\"features_2\")\n",
    "df_test = df_test.drop(\"features_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "columns_to_assemble = [\"string_array_hashed\"] + [col for col in df_train.columns if \"float\" in col]\n",
    "vect_assembler = VectorAssembler(inputCols=columns_to_assemble, outputCol=\"features_2\")\n",
    "df_train = vect_assembler.transform(df_train)\n",
    "df_val = vect_assembler.transform(df_val)\n",
    "df_test = vect_assembler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# Test Vector Assembler\n",
    "Test.assertEquals(len(columns_to_assemble), 14, \"incorrect variable columns_to_assemble\")\n",
    "Test.assertTrue(\"features_2\" in df_train.columns, \"missing features_2 col in train dataframe\")\n",
    "Test.assertTrue(\"features_2\" in df_val.columns, \"missing features_2 col in validation dataframe\")\n",
    "Test.assertTrue(\"features_2\" in df_test.columns, \"missing features_2 col in test dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Machine Learning\n",
    "\n",
    "We are now ready to train our second CTR classifier with news hashed features. \n",
    "\n",
    "- First, create a [LogisticRegression](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression) instance. Set the hyperameters such as defined in below note.\n",
    "- Then, train the machine learning algorithm and assign the resulting model to variable _model_.\n",
    "- Finally, make predictions on both train and validation set. Assign the resulting dataframe to variables **df_train_pred** and **df_val_pred**. Print the validation dataframe on the standard output using method _.show()_.\n",
    "\n",
    "_Note_ : Because we deal with a huge amount of different features, it is highly recommended to use regularization to avoid overfitting. \n",
    "\n",
    "_Note_ : Concerning logistic regression hyperparameters, use a L2 regularization with strength 10 and 2 maximum iterations and set _fitIntercept_ to True. Other hyperparameters have to be set default.\n",
    "\n",
    "_Note_ : Remember that after each action, Spark RDD and dataframes are automatically uncached. Because a machine learning algorithm is a heavy process, don't forget to `cache` again your train dataframe before training algorithm.\n",
    "\n",
    "_Note_ : Results will certainly be worth that for previous logistic regression. Do not care for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log loss on train dataset : 0.5287504103268208\n",
      "Mean log loss on validation dataset : 0.5343640827395868\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Train a logistic regression on train dataframe : \n",
    "lr = LogisticRegression(featuresCol=\"features_2\", labelCol=\"label\", fitIntercept=True, maxIter=2,\n",
    "                        elasticNetParam=1.0, regParam=10)\n",
    "df_train = df_train.cache()\n",
    "model = lr.fit(df_train)\n",
    "\n",
    "# Make prediction on validation dataframe and show it :\n",
    "df_train_pred = model.transform(df_train)\n",
    "df_val_pred = model.transform(df_val)\n",
    "\n",
    "df_train_pred_logloss = df_train_pred.withColumn(\"logloss\", my_udf(\"probability\", \"label\"))\n",
    "mean_log_loss_train = df_train_pred_logloss.select(mean(df_train_pred_logloss.logloss)).collect()[0][0]\n",
    "\n",
    "df_val_pred_logloss = df_val_pred.withColumn(\"logloss\", my_udf(\"probability\", \"label\"))\n",
    "mean_log_loss_val = df_val_pred_logloss.select(mean(df_val_pred_logloss.logloss)).collect()[0][0]\n",
    "\n",
    "print(\"Mean log loss on train dataset : %s\" % mean_log_loss_train)\n",
    "print(\"Mean log loss on validation dataset : %s\" % mean_log_loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test failed. incorect featuresCol parameter\n",
      "1 test passed.\n",
      "1 test failed. incorect maxIter parameter\n",
      "1 test passed.\n",
      "1 test failed. incorect regParam parameter\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test failed. Incorrect mean log loss for train dataset\n",
      "1 test failed. Incorrect mean log loss for validation dataset\n"
     ]
    }
   ],
   "source": [
    "# TEST Machine Learning (4.5)\n",
    "Test.assertEquals(df_train.is_cached, True, \"do not forget to cache train dataframe.\")\n",
    "Test.assertEquals(lr.getOrDefault(\"labelCol\"), \"label\", \"incorect labelCol parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"featuresCol\"), \"features_2\", \"incorect featuresCol parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"fitIntercept\"), True, \"incorect fitIntercept parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"maxIter\"), 2, \"incorect maxIter parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"elasticNetParam\"), 0.0, \"incorect elasticNetParam parameter\")\n",
    "Test.assertEquals(lr.getOrDefault(\"regParam\"), 10, \"incorect regParam parameter\")\n",
    "Test.assertTrue(\"probability\" in df_train_pred.columns, \"missing transformation for train dataframe.\")\n",
    "Test.assertTrue(\"probability\" in df_val_pred.columns, \"missing transformation for validation dataframe.\")\n",
    "Test.assertEquals(round(mean_log_loss_train, 2), 0.47, \"Incorrect mean log loss for train dataset\")\n",
    "Test.assertEquals(round(mean_log_loss_val, 2), 0.51, \"Incorrect mean log loss for validation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Design a hyperparameters tuning grid\n",
    "\n",
    "As you may have noticed, this new algorithm results are not better than the previous one. This is due to the many hyperparameters we can use to correctly set our model. To find a suitable combination for the new hased features :\n",
    "\n",
    "First, design a grid search composed of following hyperparatemers range : \n",
    "- _maxIter_ : [2]\n",
    "- _elasticNetParam_ : [0.5, 0.8]\n",
    "- _regParam_ : [0.01, 0.1, 0.5]\n",
    "\n",
    "Then, Create a _CrossValidator_ instance with :\n",
    "- parameter _estimator_ = _lr_.\n",
    "- parameter estimatorParamMaps_ = _grid_.\n",
    "- evaluator = _BinaryClassificationEvaluator(metricsName=metricName=\"areaUnderROC\")_\n",
    "\n",
    "Finally, train the grid on train dataframe and assign the followinf result to variable `cvModel`.\n",
    "\n",
    "_Note_ : You can reach information about how to build grid and CrossValidators [here](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#module-pyspark.ml.tuning).\n",
    "\n",
    "_Note_ : Log Loss function is not currently implemented as a pyspark ML evaluator. Without log loss metrics, we have to choose an other default metrics for the Grid design (AreaUnderRoc). Once we will find suitable hyperparameters, we will run the model and evaluate the log loss metrics.\n",
    "\n",
    "_Note_ : Remember that after each action, Spark RDD and dataframes are automatically uncached. Because a machine learning algorithm is a heavy process, don't forget to `cache` again your train dataframe before training algorithm.\n",
    "\n",
    "_Note_ : Grid may takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = ParamGridBuilder() \\\n",
    "     .baseOn({lr.featuresCol: 'features_2'}) \\\n",
    "     .baseOn({lr.labelCol: 'label'}) \\\n",
    "     .baseOn([lr.predictionCol, 'prediction']) \\\n",
    "     .addGrid(lr.maxIter, [2]) \\\n",
    "     .addGrid(lr.elasticNetParam, [0.5, 0.8]) \\\n",
    "     .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "     .build()\n",
    "        \n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(df_train.cache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Evaluate the grid Model\n",
    "\n",
    "Once a grid is trained using Spark CrossValidator, best model is saved and can be used using method _.transform()_. We can now make predictions for both train, validation and test datasets. \n",
    "\n",
    "- Compute these predictions and assign the resulting to variables **df_train_pred**, **df_val_pred**, **df_test_pred**.\n",
    "- Compute The mean log loss for each of these dataframes and assign the results to variables **mean_log_loss_train**, **mean_log_loss_val** and **mean_log_loss_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log loss on train dataset : 0.4261399533971181\n",
      "Mean log loss on validation dataset : 0.452331596219578\n",
      "Mean log loss on Test dataset : 0.5018769700388688\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df_train_pred = cvModel.transform(df_train)\n",
    "df_val_pred = cvModel.transform(df_val)\n",
    "df_test_pred = cvModel.transform(df_test)\n",
    "\n",
    "df_train_pred_logloss = df_train_pred.withColumn(\"logloss\", my_udf(\"probability\", \"label\"))\n",
    "mean_log_loss_train = df_train_pred_logloss.select(mean(df_train_pred_logloss.logloss)).collect()[0][0]\n",
    "\n",
    "df_val_pred_logloss = df_val_pred.withColumn(\"logloss\", my_udf(\"probability\", \"label\"))\n",
    "mean_log_loss_val = df_val_pred_logloss.select(mean(df_val_pred_logloss.logloss)).collect()[0][0]\n",
    "\n",
    "df_test_pred_logloss = df_test_pred.withColumn(\"logloss\", my_udf(\"probability\", \"label\"))\n",
    "mean_log_loss_test = df_test_pred_logloss.select(mean(df_test_pred_logloss.logloss)).collect()[0][0]\n",
    "\n",
    "print(\"Mean log loss on train dataset : %s\" % mean_log_loss_train)\n",
    "print(\"Mean log loss on validation dataset : %s\" % mean_log_loss_val)\n",
    "print(\"Mean log loss on Test dataset : %s\" % mean_log_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(round(mean_log_loss_train, 2), 0.43, \"Incorrect mean log loss for train dataset\")\n",
    "Test.assertEquals(round(mean_log_loss_val, 2), 0.45, \"Incorrect mean log loss for validation dataset\")\n",
    "Test.assertEquals(round(mean_log_loss_test, 2), 0.50, \"Incorrect mean log loss for test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "you have now reached the end of the lab. You can compare your final score with the offical Kaggle competition leaderboard. Keep in mind you have only used a small subset of data and a really simple logistic regression mode. Using an Hadoop cluster, you would be able to adress these issues.\n",
    "\n",
    "Kaggle Learderboard : https://www.kaggle.com/c/criteo-display-ad-challenge/leaderboard"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3Spark (Spark 2.2.0)",
   "language": "python",
   "name": "py3spark2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
